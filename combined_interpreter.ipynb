{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8e4cb982-03fb-4982-ab38-312b2d644be3",
   "metadata": {},
   "source": [
    "# Reconocedor de lenguaje de señas Argentino entrenado solo con el dataset argentino."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bc89cb28-63ce-4021-bc45-de4b92ea4a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications.inception_v3 import InceptionV3, preprocess_input, decode_predictions\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D, Dense, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb3b6fc2-2c8a-49e0-8a50-7ca8db986aab",
   "metadata": {},
   "source": [
    "Importamos los datasets y hacemos un split.\n",
    "Comencemos por el dataset mas populado, el de lenguaje de señas americano, que utilizaremos para entrenar las capas intermedias.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "56e5079e-6849-450b-a250-2c1e2e3722a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2515 files belonging to 36 classes.\n",
      "Using 2012 files for training.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-01 19:10:51.575782: E external/local_xla/xla/stream_executor/cuda/cuda_platform.cc:51] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (303)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2515 files belonging to 36 classes.\n",
      "Using 503 files for validation.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing import image_dataset_from_directory\n",
    "\n",
    "# Definimos los parametros\n",
    "image_size = (299, 299)\n",
    "batch_size = 32\n",
    "asl_dir = \"asl_dataset/\"\n",
    "train_val_seed = 42        # Es importante que sea la misma para ambos llamados\n",
    "\n",
    "# Y creamos los conjuntos de entrenamiento y validacion. \n",
    "# Esto es medio raro, porque invocamos dos veces a image_dataset_from_directory para hacer el split,\n",
    "# pero es la manera que indica la documentacion\n",
    "asl_train_ds = image_dataset_from_directory(\n",
    "    asl_dir,\n",
    "    validation_split=0.2,\n",
    "    subset=\"training\",\n",
    "    seed=train_val_seed, \n",
    "    image_size=image_size,\n",
    "    batch_size=batch_size,\n",
    "    label_mode='int'   # or 'categorical' if you want one-hot\n",
    ")\n",
    "\n",
    "asl_val_ds = image_dataset_from_directory(\n",
    "    asl_dir,\n",
    "    validation_split=0.2,\n",
    "    subset=\"validation\",\n",
    "    seed=train_val_seed, \n",
    "    image_size=image_size,\n",
    "    batch_size=batch_size,\n",
    "    label_mode='int'\n",
    ")\n",
    "\n",
    "class_names = asl_train_ds.class_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "716a4e2e-747b-494f-a8d7-7cce626f49bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-01 19:10:58.748535: W external/local_xla/xla/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 34329984 exceeds 10% of free system memory.\n",
      "2025-08-01 19:10:58.767188: W external/local_xla/xla/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 34329984 exceeds 10% of free system memory.\n",
      "2025-08-01 19:10:58.784858: W external/local_xla/xla/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 34329984 exceeds 10% of free system memory.\n",
      "2025-08-01 19:10:58.812616: W external/local_xla/xla/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 34329984 exceeds 10% of free system memory.\n",
      "2025-08-01 19:10:58.838555: W external/local_xla/xla/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 34329984 exceeds 10% of free system memory.\n",
      "2025-08-01 19:11:00.128875: I tensorflow/core/framework/local_rendezvous.cc:407] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entradas en entrenamiento: 2012\n",
      "Entradas en validación: 503\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-01 19:11:00.533292: I tensorflow/core/framework/local_rendezvous.cc:407] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'asl_test_ds' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 12\u001b[39m\n\u001b[32m     10\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mEntradas en entrenamiento:\u001b[39m\u001b[33m\"\u001b[39m, count_elements(asl_train_ds))\n\u001b[32m     11\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mEntradas en validación:\u001b[39m\u001b[33m\"\u001b[39m, count_elements(asl_val_ds))\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mEntradas en test:\u001b[39m\u001b[33m\"\u001b[39m, count_elements(\u001b[43masl_test_ds\u001b[49m))\n",
      "\u001b[31mNameError\u001b[39m: name 'asl_test_ds' is not defined"
     ]
    }
   ],
   "source": [
    "print(class_names)\n",
    "\n",
    "def count_elements(dataset):\n",
    "    count = 0\n",
    "    for batch in dataset:\n",
    "        images, labels = batch\n",
    "        count += images.shape[0]\n",
    "    return count\n",
    "\n",
    "print(\"Entradas en entrenamiento:\", count_elements(asl_train_ds))\n",
    "print(\"Entradas en validación:\", count_elements(asl_val_ds))\n",
    "print(\"Entradas en test:\", count_elements(asl_test_ds))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45717ea2-f834-47a6-bab2-b488ac7ab166",
   "metadata": {},
   "source": [
    "Procesamos ahora las imagenes para adecuarlas al formato de *InceptionV3*,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d9112aa8-3506-40e4-8db6-641dc5a0ae74",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications.inception_v3 import preprocess_input\n",
    "\n",
    "def preprocess_img(image, label):\n",
    "    image = preprocess_input(image) \n",
    "    return image, label\n",
    "\n",
    "asl_train_ds = asl_train_ds.map(preprocess_img).prefetch(tf.data.AUTOTUNE)\n",
    "asl_val_ds   = asl_val_ds.map(preprocess_img).prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5f7e137-0e04-42fd-ad69-a60e6359e1f2",
   "metadata": {},
   "source": [
    "Chequeemos que obtuvimos las clases correctas,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "057ccc05-bd37-4206-b13c-7604eebce6ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n"
     ]
    }
   ],
   "source": [
    "print(class_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "818453ec-a778-49c3-a184-945d0d4fe35d",
   "metadata": {},
   "source": [
    "Continuemos con la carga del dataset de lenguaje de señas argentino que definira las clases sobre la que predecirá el modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9ab254a5-4edd-4466-8124-9d626f4d6421",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Extraemos los nombres de todas las imagenes que vamos a utilizar\n",
    "lsa_dir = 'lsa16_segmented/'\n",
    "filenames = [f for f in os.listdir(lsa_dir)]\n",
    "\n",
    "# Y de cada una extraemos su clase, que viene dada por el primer numero del nombre\n",
    "labels = [int(f.split('_')[0]) - 1 for f in filenames]   # Le restamos 1 a los labels para que esten en rango [0, 16) en vez de [1, 16]\n",
    "\n",
    "# Y creamos un dataframe que asocia a cada nombre de archivo su clase.\n",
    "lsa_df = pd.DataFrame({'filename': filenames, 'class': labels})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c35cdbe3-7c07-472a-a3e6-32f951d47104",
   "metadata": {},
   "source": [
    "Preprocesamos las imagenes para adecuarlas al formato de ImageNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "533ae972-308d-447c-8a46-927dc74b94e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Separaramos el conjunto de test (20%)\n",
    "lsa_temp_df, lsa_test_df = train_test_split(\n",
    "    lsa_df,\n",
    "    test_size=0.2,\n",
    "    stratify=lsa_df['class'],\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Y separamos lo restante en train y val (80% train -> 64% total, 20% val -> 16% total)\n",
    "lsa_train_df, lsa_val_df = train_test_split(\n",
    "    lsa_temp_df,\n",
    "    test_size=0.2,  # 20% de 80% = 16% del total\n",
    "    stratify=lsa_temp_df['class'],\n",
    "    random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8025e1e3-4e1e-4969-9039-437b909fddb7",
   "metadata": {},
   "source": [
    "Creamos un *pipeline* de datos de *TensorFlow*. La idea es aprovechar la paralelización del *map* para procesar los datos mas rápido."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "325d2a59-b2f9-46b4-b97d-0d2c99eafe37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definimos una funcion que dado un filename devuelve su imagen y su clase o label\n",
    "def load_and_preprocess(image_path, label):\n",
    "\n",
    "    # Leemos el archivo y lo decodificamos en RGB\n",
    "    img = tf.io.read_file(lsa_dir + image_path)\n",
    "    img = tf.image.decode_jpeg(img, channels=3) \n",
    "    \n",
    "    # Lo preprocesamos para InceptionV3\n",
    "    img = tf.image.resize(img, [299, 299])\n",
    "    img = preprocess_input(img)  # Obs. que preprocess_input es una funcion de inception_v3 en particular\n",
    "    \n",
    "    return img, label\n",
    "\n",
    "# Usamos un batch_size de TensorFlow estandar\n",
    "batch_size = 32\n",
    "\n",
    "# 1. Cargamos el dataframe\n",
    "lsa_ds = tf.data.Dataset.from_tensor_slices((lsa_train_df['filename'].values, lsa_train_df['class'].values))\n",
    "\n",
    "# 2. Le mappeamos el preprocesamiento a cada entrada, paralelizando\n",
    "lsa_ds = lsa_ds.map(load_and_preprocess, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "\n",
    "# 3. Mezclamos para randomizar el orden de las muestras\n",
    "lsa_ds = lsa_ds.shuffle(buffer_size=len(lsa_train_df))\n",
    "\n",
    "# 4. Usamos el batch_size estandar\n",
    "lsa_ds = lsa_ds.batch(batch_size)\n",
    "\n",
    "# 5. Permitimos el prefetching del proximo batch\n",
    "lsa_train_ds = lsa_ds.prefetch(tf.data.AUTOTUNE)                                                       \n",
    "\n",
    "# Y repetimos lo mismo para el conjunto de validacion\n",
    "lsa_val_ds = tf.data.Dataset.from_tensor_slices((lsa_val_df['filename'].values, lsa_val_df['class'].values))\\\n",
    "           .map(load_and_preprocess, num_parallel_calls=tf.data.AUTOTUNE) \\\n",
    "           .batch(batch_size) \\\n",
    "           .prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c53e367-6eaa-45a3-8adf-be391a16b7a5",
   "metadata": {},
   "source": [
    "Entrenemos ahora las capas intermedias del modelo con el *dataset* de ASL,"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5455f5a-97ea-42f3-b84d-f05dbbb3a31e",
   "metadata": {},
   "source": [
    "Cargamos el modelo de manera que sea entrenable. No incluímos su última capa para poder establecer nuestras propias clases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c215d95e-eb10-42fa-8563-d2cfc2196a0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-01 19:12:04.280045: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 34330112 bytes after encountering the first element of size 34330112 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m191s\u001b[0m 3s/step - accuracy: 0.0934 - loss: 3.4406 - val_accuracy: 0.1829 - val_loss: 3.3185\n",
      "Epoch 2/5\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m179s\u001b[0m 3s/step - accuracy: 0.3941 - loss: 2.7724 - val_accuracy: 0.6223 - val_loss: 2.6404\n",
      "Epoch 3/5\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m174s\u001b[0m 3s/step - accuracy: 0.6481 - loss: 2.1396 - val_accuracy: 0.8509 - val_loss: 1.8960\n",
      "Epoch 4/5\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m174s\u001b[0m 3s/step - accuracy: 0.7987 - loss: 1.5833 - val_accuracy: 0.8986 - val_loss: 1.2799\n",
      "Epoch 5/5\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m179s\u001b[0m 3s/step - accuracy: 0.8693 - loss: 1.1402 - val_accuracy: 0.9245 - val_loss: 0.8667\n"
     ]
    }
   ],
   "source": [
    "# Cargamos InceptionV3\n",
    "base_model = InceptionV3(weights = 'imagenet',       # Pre-entrenado con ImageNet\n",
    "                         include_top = False,        # Sin incluir su capa de clasificacion con 1000 clases para poder hacer fine-tuning \n",
    "                         input_shape = (299, 299, 3) # Necesario cuando no incluimos la ultima capa\n",
    "                        )\n",
    "\n",
    "# Inicialmente descongelamos todas las capas, despues congelamos las que no queremos que se entrenen\n",
    "base_model.trainable = True\n",
    "\n",
    "# Descongelamos desde la capa llamada mixed7, lo que descongela las ultimas ~50 capas.\n",
    "set_trainable = False\n",
    "for layer in base_model.layers:\n",
    "    if layer.name == \"mixed7\":\n",
    "        set_trainable = True\n",
    "    layer.trainable = set_trainable\n",
    "\n",
    "# Construimos la cabeza de clasificacion para las 36 clases de ASL\n",
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dense(1024, activation='relu')(x)\n",
    "x = Dropout(0.5)(x)\n",
    "predictions = Dense(36, activation='softmax')(x)  # 26 letras + 10 digitos\n",
    "\n",
    "model = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "# Compilamos el modelo usando un learning_rate bajo.\n",
    "tuning_learning_rate = 1e-5\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=tuning_learning_rate),\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Lo entrenamos con esos datos\n",
    "model.fit(asl_train_ds, validation_data=asl_val_ds, epochs=5)\n",
    "\n",
    "# Y nos guardamos los pesos del modelo del cual luego usaremos todo menos la cabeza de clasificacion.\n",
    "model.save_weights(\"inceptionv3_hand_features.weights.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "310a01d8-b66f-45c4-ba3a-1cb6a2036885",
   "metadata": {},
   "source": [
    "Ahora cargamos ese modelo que entrenamos pero le sacamos la cabeza y colocamos la clasificadora de LSA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cba92cd4-e9bf-4027-9f6d-18365fc31c11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/octavio/envs/asl_env/lib/python3.12/site-packages/keras/src/saving/saving_lib.py:643: UserWarning: A total of 1 objects could not be loaded. Example error message for object <Dense name=dense_3, built=True>:\n",
      "\n",
      "The shape of the target variable and the shape of the target value in `variable.assign(value)` must match. variable.shape=(1024, 16), Received: value.shape=(1024, 36). Target variable: <Variable path=dense_3/kernel, shape=(1024, 16), dtype=float32, value=[[ 0.04093459  0.04401912 -0.04039394 ...  0.0559013  -0.01975827\n",
      "  -0.0489627 ]\n",
      " [-0.03034027 -0.03789191 -0.07005539 ...  0.01434596 -0.03185568\n",
      "  -0.02784391]\n",
      " [-0.07448781 -0.03328026  0.02478389 ...  0.04706521 -0.04614044\n",
      "   0.00913257]\n",
      " ...\n",
      " [-0.02381406  0.03114007  0.01424659 ...  0.00447538 -0.03607813\n",
      "   0.02731498]\n",
      " [ 0.0759483   0.04060939  0.07290486 ...  0.01610276 -0.07496329\n",
      "  -0.06972116]\n",
      " [ 0.06632379  0.03647552  0.04533759 ... -0.04951165 -0.03469245\n",
      "   0.02361427]]>\n",
      "\n",
      "List of objects that could not be loaded:\n",
      "[<Dense name=dense_3, built=True>]\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 2s/step - accuracy: 0.1426 - loss: 3.0731 - val_accuracy: 0.4141 - val_loss: 2.0438\n",
      "Epoch 2/10\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 2s/step - accuracy: 0.4355 - loss: 1.7786 - val_accuracy: 0.5781 - val_loss: 1.4771\n",
      "Epoch 3/10\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 2s/step - accuracy: 0.5801 - loss: 1.3073 - val_accuracy: 0.6094 - val_loss: 1.1665\n",
      "Epoch 4/10\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 2s/step - accuracy: 0.6934 - loss: 0.9759 - val_accuracy: 0.7344 - val_loss: 0.9432\n",
      "Epoch 5/10\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 2s/step - accuracy: 0.7598 - loss: 0.7740 - val_accuracy: 0.7500 - val_loss: 0.8252\n",
      "Epoch 6/10\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 2s/step - accuracy: 0.8359 - loss: 0.5942 - val_accuracy: 0.7656 - val_loss: 0.7744\n",
      "Epoch 7/10\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 2s/step - accuracy: 0.8691 - loss: 0.4691 - val_accuracy: 0.7812 - val_loss: 0.7489\n",
      "Epoch 8/10\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 2s/step - accuracy: 0.8672 - loss: 0.4660 - val_accuracy: 0.7734 - val_loss: 0.7197\n",
      "Epoch 9/10\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 2s/step - accuracy: 0.9141 - loss: 0.3484 - val_accuracy: 0.7812 - val_loss: 0.6553\n",
      "Epoch 10/10\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 2s/step - accuracy: 0.9121 - loss: 0.3315 - val_accuracy: 0.7578 - val_loss: 0.6792\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x7d84243ce720>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reconstruimos el modelo, nuevamente sin incluir el top.\n",
    "base_model = InceptionV3(weights=None, include_top=False, input_shape=(299, 299, 3))\n",
    "base_model.trainable = False  # Y en este caso freezamos todas las capas pues solo queremos entrenar la que agregaremos\n",
    "\n",
    "# Le agregamos la ultima capa\n",
    "num_classes = lsa_train_df['class'].nunique()\n",
    "\n",
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dense(1024, activation='relu')(x)\n",
    "x = Dropout(0.5)(x)\n",
    "predictions = Dense(num_classes, activation='softmax')(x)\n",
    "\n",
    "model = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "# Cargamos solo las capas compartidas con el modelo que entrenamos antes\n",
    "model.load_weights(\"inceptionv3_hand_features.weights.h5\", skip_mismatch=True)\n",
    "\n",
    "# Lo compilamos, ahora con un learning rate un poco mas alto.\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Y entrenamos\n",
    "model.fit(lsa_train_ds, validation_data=lsa_val_ds, epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54e976c2-6fff-498a-ba1a-d96c5ebdcb9e",
   "metadata": {},
   "source": [
    "Y guardamos los pesos de este modelo final,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c164c3b6-1a99-45b8-8ff3-4e58386c511f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights(\"combined_interpreter_final.weights.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f1c534d-eeaa-4ef1-838b-48e1ee355ecc",
   "metadata": {},
   "source": [
    "Esto que sigue creo que no funciona."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bb8c428a-b615-495c-b649-a06188a5b7e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4s/step\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "`decode_predictions` expects a batch of predictions (i.e. a 2D array of shape (samples, 1000)). Received array with shape: (1, 16)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# Predict\u001b[39;00m\n\u001b[32m      8\u001b[39m predictions = model.predict(img_array)\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m decoded_predictions = \u001b[43mdecode_predictions\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpredictions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m5\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[32m0\u001b[39m]\n\u001b[32m     11\u001b[39m \u001b[38;5;66;03m# Display results\u001b[39;00m\n\u001b[32m     12\u001b[39m plt.imshow(img)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/envs/asl_env/lib/python3.12/site-packages/keras/src/applications/inception_v3.py:434\u001b[39m, in \u001b[36mdecode_predictions\u001b[39m\u001b[34m(preds, top)\u001b[39m\n\u001b[32m    432\u001b[39m \u001b[38;5;129m@keras_export\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mkeras.applications.inception_v3.decode_predictions\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    433\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecode_predictions\u001b[39m(preds, top=\u001b[32m5\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m434\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimagenet_utils\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdecode_predictions\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpreds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtop\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/envs/asl_env/lib/python3.12/site-packages/keras/src/applications/imagenet_utils.py:136\u001b[39m, in \u001b[36mdecode_predictions\u001b[39m\u001b[34m(preds, top)\u001b[39m\n\u001b[32m    133\u001b[39m \u001b[38;5;28;01mglobal\u001b[39;00m CLASS_INDEX\n\u001b[32m    135\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(preds.shape) != \u001b[32m2\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m preds.shape[\u001b[32m1\u001b[39m] != \u001b[32m1000\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m136\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    137\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m`decode_predictions` expects \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    138\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33ma batch of predictions \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    139\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m(i.e. a 2D array of shape (samples, 1000)). \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    140\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mReceived array with shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpreds.shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    141\u001b[39m     )\n\u001b[32m    142\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m CLASS_INDEX \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    143\u001b[39m     fpath = file_utils.get_file(\n\u001b[32m    144\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mimagenet_class_index.json\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    145\u001b[39m         CLASS_INDEX_PATH,\n\u001b[32m    146\u001b[39m         cache_subdir=\u001b[33m\"\u001b[39m\u001b[33mmodels\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    147\u001b[39m         file_hash=\u001b[33m\"\u001b[39m\u001b[33mc2c37ea517e94d9795004a39431a14cb\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    148\u001b[39m     )\n",
      "\u001b[31mValueError\u001b[39m: `decode_predictions` expects a batch of predictions (i.e. a 2D array of shape (samples, 1000)). Received array with shape: (1, 16)"
     ]
    }
   ],
   "source": [
    "img_path = 'lsa16_segmented/1_1_1.png'  \n",
    "img = image.load_img(img_path, target_size=(299, 299)) # La carga en img y le hace resize a 299x299\n",
    "img_array = image.img_to_array(img)                    # La convierte a array de NumPy con dimensiones (299, 299, 3)\n",
    "img_array = np.expand_dims(img_array, axis=0)          # Agrega una dimension mas al array, haciendolo (1, 299, 299, 3) para batching\n",
    "img_array = preprocess_input(img_array)                # Matchea la representacion de la imagen a como la espera ImageNet (ej. mappea 0-255 a -1,1, cambia de RGB a BGR)\n",
    "\n",
    "# Predict\n",
    "predictions = model.predict(img_array)\n",
    "decoded_predictions = decode_predictions(predictions, top=5)[0]\n",
    "\n",
    "# Display results\n",
    "plt.imshow(img)\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "print(\"Top 5 Predictions:\")\n",
    "for i, (_, label, prob) in enumerate(decoded_predictions):\n",
    "    print(f\"{i + 1}: {label} ({prob * 100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6210e17d-f1dd-45bd-a9fb-79a79135dbf6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (NEW ASL Env)",
   "language": "python",
   "name": "asl_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
