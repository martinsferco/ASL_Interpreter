{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8e4cb982-03fb-4982-ab38-312b2d644be3",
   "metadata": {},
   "source": [
    "# Reconocedor de lenguaje de señas Argentino entrenado solo con el dataset argentino."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bc89cb28-63ce-4021-bc45-de4b92ea4a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications.inception_v3 import InceptionV3, preprocess_input, decode_predictions\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D, Dense, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5455f5a-97ea-42f3-b84d-f05dbbb3a31e",
   "metadata": {},
   "source": [
    "Cargamos el modelo de manera que sea entrenable. No incluímos su última capa para poder establecer nuestras propias clases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e3bcd231-49d4-440f-9fdb-6b49a9d8bd68",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cb3b6fc2-2c8a-49e0-8a50-7ca8db986aab",
   "metadata": {},
   "source": [
    "Importamos los datasets y hacemos un split.\n",
    "Comencemos por el dataset mas populado, el de lenguaje de señas americano, que utilizaremos para entrenar las capas intermedias.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "56e5079e-6849-450b-a250-2c1e2e3722a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2515 files belonging to 36 classes.\n",
      "Using 2012 files for training.\n",
      "Found 2515 files belonging to 36 classes.\n",
      "Using 503 files for validation.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing import image_dataset_from_directory\n",
    "\n",
    "# Definimos los parametros\n",
    "image_size = (299, 299)\n",
    "batch_size = 32\n",
    "asl_dir = \"asl_dataset/\"\n",
    "train_val_seed = 42        # Es importante que sea la misma para ambos llamados\n",
    "\n",
    "# Y creamos los conjuntos de entrenamiento y validacion. \n",
    "# Esto es medio raro, porque invocamos dos veces a image_dataset_from_directory para hacer el split,\n",
    "# pero es la manera que indica la documentacion\n",
    "asl_train_ds = image_dataset_from_directory(\n",
    "    asl_dir,\n",
    "    validation_split=0.2,\n",
    "    subset=\"training\",\n",
    "    seed=train_val_seed, \n",
    "    image_size=image_size,\n",
    "    batch_size=batch_size,\n",
    "    label_mode='int'   # or 'categorical' if you want one-hot\n",
    ")\n",
    "\n",
    "asl_val_ds = image_dataset_from_directory(\n",
    "    asl_dir,\n",
    "    validation_split=0.2,\n",
    "    subset=\"validation\",\n",
    "    seed=train_val_seed, \n",
    "    image_size=image_size,\n",
    "    batch_size=batch_size,\n",
    "    label_mode='int'\n",
    ")\n",
    "\n",
    "class_names = asl_train_ds.class_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45717ea2-f834-47a6-bab2-b488ac7ab166",
   "metadata": {},
   "source": [
    "Procesamos ahora las imagenes para adecuarlas al formato de *InceptionV3*,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d9112aa8-3506-40e4-8db6-641dc5a0ae74",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications.inception_v3 import preprocess_input\n",
    "\n",
    "def preprocess_img(image, label):\n",
    "    image = preprocess_input(image) \n",
    "    return image, label\n",
    "\n",
    "asl_train_ds = asl_train_ds.map(preprocess_img).prefetch(tf.data.AUTOTUNE)\n",
    "asl_val_ds   = asl_val_ds.map(preprocess_img).prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5f7e137-0e04-42fd-ad69-a60e6359e1f2",
   "metadata": {},
   "source": [
    "Chequeemos que obtuvimos las clases correctas,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "057ccc05-bd37-4206-b13c-7604eebce6ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n"
     ]
    }
   ],
   "source": [
    "print(class_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "818453ec-a778-49c3-a184-945d0d4fe35d",
   "metadata": {},
   "source": [
    "Continuemos con la carga del dataset de lenguaje de señas argentino que definira las clases sobre la que predecirá el modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9ab254a5-4edd-4466-8124-9d626f4d6421",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Extraemos los nombres de todas las imagenes que vamos a utilizar\n",
    "lsa_dir = 'lsa16_segmented/'\n",
    "filenames = [f for f in os.listdir(image_dir)]\n",
    "\n",
    "# Y de cada una extraemos su clase, que viene dada por el primer numero del nombre\n",
    "labels = [int(f.split('_')[0]) - 1 for f in filenames]   # Le restamos 1 a los labels para que esten en rango [0, 16) en vez de [1, 16]\n",
    "\n",
    "# Y creamos un dataframe que asocia a cada nombre de archivo su clase.\n",
    "lsa_df = pd.DataFrame({'filename': filenames, 'class': labels})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c35cdbe3-7c07-472a-a3e6-32f951d47104",
   "metadata": {},
   "source": [
    "Preprocesamos las imagenes para adecuarlas al formato de ImageNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "533ae972-308d-447c-8a46-927dc74b94e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separamos al dataset en train y validacion.\n",
    "lsa_train_df, lsa_val_df = train_test_split(lsa_df,\n",
    "                                    test_size=0.2,\n",
    "                                    stratify=df['class'],   # Hace que se mantengan las proporciones de las clases luego del split\n",
    "                                    random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8025e1e3-4e1e-4969-9039-437b909fddb7",
   "metadata": {},
   "source": [
    "Creamos un *pipeline* de datos de *TensorFlow*. La idea es aprovechar la paralelización del *map* para procesar los datos mas rápido."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "325d2a59-b2f9-46b4-b97d-0d2c99eafe37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definimos una funcion que dado un filename devuelve su imagen y su clase o label\n",
    "def load_and_preprocess(image_path, label):\n",
    "\n",
    "    # Leemos el archivo y lo decodificamos en RGB\n",
    "    img = tf.io.read_file(image_dir + image_path)\n",
    "    img = tf.image.decode_jpeg(img, channels=3) \n",
    "    \n",
    "    # Lo preprocesamos para InceptionV3\n",
    "    img = tf.image.resize(img, [299, 299])\n",
    "    img = preprocess_input(img)  # Obs. que preprocess_input es una funcion de inception_v3 en particular\n",
    "    \n",
    "    return img, label\n",
    "\n",
    "# Usamos un batch_size de TensorFlow estandar\n",
    "batch_size = 32\n",
    "\n",
    "# 1. Cargamos el dataframe\n",
    "lsa_ds = tf.data.Dataset.from_tensor_slices((lsa_train_df['filename'].values, lsa_train_df['class'].values))\n",
    "\n",
    "# 2. Le mappeamos el preprocesamiento a cada entrada, paralelizando\n",
    "lsa_ds = lsa_ds.map(load_and_preprocess, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "\n",
    "# 3. Mezclamos para randomizar el orden de las muestras\n",
    "lsa_ds = lsa_ds.shuffle(buffer_size=len(lsa_train_df))\n",
    "\n",
    "# 4. Usamos el batch_size estandar\n",
    "lsa_ds = lsa_ds.batch(batch_size)\n",
    "\n",
    "# 5. Permitimos el prefetching del proximo batch\n",
    "lsa_train_ds = lsa_ds.prefetch(tf.data.AUTOTUNE)                                                       \n",
    "\n",
    "# Y repetimos lo mismo para el conjunto de validacion\n",
    "lsa_val_ds = tf.data.Dataset.from_tensor_slices((lsa_val_df['filename'].values, lsa_val_df['class'].values))\\\n",
    "           .map(load_and_preprocess, num_parallel_calls=tf.data.AUTOTUNE) \\\n",
    "           .batch(batch_size) \\\n",
    "           .prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c53e367-6eaa-45a3-8adf-be391a16b7a5",
   "metadata": {},
   "source": [
    "Entrenemos ahora las capas intermedias del modelo con el *dataset* de ASL,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c215d95e-eb10-42fa-8563-d2cfc2196a0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m187s\u001b[0m 3s/step - accuracy: 0.0514 - loss: 3.6089 - val_accuracy: 0.1590 - val_loss: 3.3617\n",
      "Epoch 2/5\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m171s\u001b[0m 3s/step - accuracy: 0.3401 - loss: 2.9007 - val_accuracy: 0.5706 - val_loss: 2.6478\n",
      "Epoch 3/5\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m190s\u001b[0m 3s/step - accuracy: 0.6118 - loss: 2.2717 - val_accuracy: 0.8310 - val_loss: 1.8617\n",
      "Epoch 4/5\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m174s\u001b[0m 3s/step - accuracy: 0.7797 - loss: 1.6661 - val_accuracy: 0.9125 - val_loss: 1.2535\n",
      "Epoch 5/5\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m172s\u001b[0m 3s/step - accuracy: 0.8633 - loss: 1.2156 - val_accuracy: 0.9423 - val_loss: 0.8423\n"
     ]
    }
   ],
   "source": [
    "# Cargamos InceptionV3\n",
    "base_model = InceptionV3(weights = 'imagenet',       # Pre-entrenado con ImageNet\n",
    "                         include_top = False,        # Sin incluir su capa de clasificacion con 1000 clases para poder hacer fine-tuning \n",
    "                         input_shape = (299, 299, 3) # Necesario cuando no incluimos la ultima capa\n",
    "                        )\n",
    "\n",
    "# Inicialmente descongelamos todas las capas, despues congelamos las que no queremos que se entrenen\n",
    "base_model.trainable = True\n",
    "\n",
    "# Descongelamos desde la capa llamada mixed7, lo que descongela las ultimas ~50 capas.\n",
    "set_trainable = False\n",
    "for layer in base_model.layers:\n",
    "    if layer.name == \"mixed7\":\n",
    "        set_trainable = True\n",
    "    layer.trainable = set_trainable\n",
    "\n",
    "# Construimos la cabeza de clasificacion para las 36 clases de ASL\n",
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dense(1024, activation='relu')(x)\n",
    "x = Dropout(0.5)(x)\n",
    "predictions = Dense(36, activation='softmax')(x)  # 26 letras + 10 digitos\n",
    "\n",
    "model = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "# Compilamos el modelo usando un learning_rate bajo.\n",
    "tuning_learning_rate = 1e-5\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=tuning_learning_rate),\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Lo entrenamos con esos datos\n",
    "model.fit(asl_train_ds, validation_data=asl_val_ds, epochs=5)\n",
    "\n",
    "# Y nos guardamos los pesos del modelo del cual luego usaremos todo menos la cabeza de clasificacion.\n",
    "model.save_weights(\"inceptionv3_hand_features.weights.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "310a01d8-b66f-45c4-ba3a-1cb6a2036885",
   "metadata": {},
   "source": [
    "Ahora cargamos ese modelo que entrenamos pero le sacamos la cabeza y colocamos la clasificadora de LSA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "cba92cd4-e9bf-4027-9f6d-18365fc31c11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/octavio/materias/ia/tp_decision_trees/ia_venv/lib/python3.12/site-packages/keras/src/saving/saving_lib.py:648: UserWarning: A total of 1 objects could not be loaded. Example error message for object <Dense name=dense_11, built=True>:\n",
      "\n",
      "The shape of the target variable and the shape of the target value in `variable.assign(value)` must match. variable.shape=(1024, 16), Received: value.shape=(1024, 36). Target variable: <Variable path=dense_11/kernel, shape=(1024, 16), dtype=float32, value=[[-0.02250922 -0.02932912  0.0746019  ...  0.02493007 -0.06474213\n",
      "  -0.03626624]\n",
      " [-0.03921478 -0.01528396 -0.03929701 ... -0.0538302  -0.05247301\n",
      "  -0.0546594 ]\n",
      " [-0.02866591  0.02207758  0.05660126 ...  0.05759862  0.0455526\n",
      "  -0.01739256]\n",
      " ...\n",
      " [ 0.03058823  0.06481682  0.06096706 ...  0.06537391 -0.02342306\n",
      "   0.03972448]\n",
      " [-0.01540674 -0.02526372  0.04881488 ...  0.04961404 -0.0309973\n",
      "  -0.07006078]\n",
      " [-0.01786763 -0.01516022  0.05429    ... -0.02224328  0.03768231\n",
      "  -0.01313288]]>\n",
      "\n",
      "List of objects that could not be loaded:\n",
      "[<Dense name=dense_11, built=True>]\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 2s/step - accuracy: 0.1082 - loss: 3.2995 - val_accuracy: 0.4875 - val_loss: 1.7997\n",
      "Epoch 2/10\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 2s/step - accuracy: 0.4875 - loss: 1.6609 - val_accuracy: 0.5688 - val_loss: 1.2980\n",
      "Epoch 3/10\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 2s/step - accuracy: 0.6298 - loss: 1.1986 - val_accuracy: 0.7125 - val_loss: 0.9809\n",
      "Epoch 4/10\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 2s/step - accuracy: 0.7540 - loss: 0.8374 - val_accuracy: 0.7000 - val_loss: 0.9027\n",
      "Epoch 5/10\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 2s/step - accuracy: 0.7662 - loss: 0.7185 - val_accuracy: 0.7125 - val_loss: 0.8306\n",
      "Epoch 6/10\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 2s/step - accuracy: 0.8582 - loss: 0.5472 - val_accuracy: 0.8062 - val_loss: 0.7289\n",
      "Epoch 7/10\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 2s/step - accuracy: 0.8784 - loss: 0.4445 - val_accuracy: 0.7437 - val_loss: 0.7134\n",
      "Epoch 8/10\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 2s/step - accuracy: 0.9121 - loss: 0.3536 - val_accuracy: 0.7437 - val_loss: 0.7005\n",
      "Epoch 9/10\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 3s/step - accuracy: 0.9077 - loss: 0.3743 - val_accuracy: 0.7875 - val_loss: 0.6136\n",
      "Epoch 10/10\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 2s/step - accuracy: 0.9297 - loss: 0.2813 - val_accuracy: 0.8000 - val_loss: 0.6289\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x7c744ee4f410>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reconstruimos el modelo, nuevamente sin incluir el top.\n",
    "base_model = InceptionV3(weights=None, include_top=False, input_shape=(299, 299, 3))\n",
    "base_model.trainable = False  # Y en este caso freezamos todas las capas pues solo queremos entrenar la que agregaremos\n",
    "\n",
    "# Le agregamos la ultima capa\n",
    "num_classes = lsa_train_df['class'].nunique()\n",
    "\n",
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dense(1024, activation='relu')(x)\n",
    "x = Dropout(0.5)(x)\n",
    "predictions = Dense(num_classes, activation='softmax')(x)\n",
    "\n",
    "model = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "# Cargamos solo las capas compartidas con el modelo que entrenamos antes\n",
    "model.load_weights(\"inceptionv3_hand_features.weights.h5\", skip_mismatch=True)\n",
    "\n",
    "# Lo compilamos, ahora con un learning rate un poco mas alto.\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Y entrenamos\n",
    "model.fit(lsa_train_ds, validation_data=lsa_val_ds, epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a41adbb9-5c5f-4dc4-9594-5f04953a7815",
   "metadata": {},
   "source": [
    "Ahora definimos la que sera nuestra ultima capa de manera que clasifique sobre la cantidad de clases que nos interesa. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8d17b423-1f74-43f7-aee1-c8656a95c5eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número de clases: 16\n"
     ]
    }
   ],
   "source": [
    "# Definimos la ultima capa para que prediga acorde a nuestras clases\n",
    "num_classes = lsa_train_df['class'].nunique()\n",
    "print(f\"Número de clases: {num_classes}\")\n",
    "\n",
    "x = base_model.output  # x es la salida del modelo hasta ahora\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dense(1024, activation='relu')(x)\n",
    "x = Dropout(0.5)(x)  # Regularización para evitar overfitting\n",
    "\n",
    "# Y creamos una nueva capa de salida que tome como input a la anterior y clasifique en num_classes clases\n",
    "predictions = Dense(num_classes, activation='softmax')(x)  \n",
    "\n",
    "\n",
    "model = Model(inputs=base_model.input, outputs=predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19653511-4330-4896-95f0-4fb553ef9bd5",
   "metadata": {},
   "source": [
    "Configuramos el modelo usando momento adaptativo y *sparse categorial cross-entropy* pues es adecuada para clasificacion multiclase con enteros según la documentación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "063b0a9d-7844-453f-913a-bd956fe5e805",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "    loss='sparse_categorical_crossentropy',  # Para etiquetas enteras\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be07eeb3-a176-4532-88d2-6676c692e191",
   "metadata": {},
   "source": [
    "Entrenemos el modelo con los nuevos datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dbd7617d-2e83-4aa9-b523-3445b13a500a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "20/20 - 51s - 3s/step - accuracy: 0.2328 - loss: 2.7291 - val_accuracy: 0.4812 - val_loss: 1.6282\n",
      "Epoch 2/10\n",
      "20/20 - 44s - 2s/step - accuracy: 0.5891 - loss: 1.3294 - val_accuracy: 0.5938 - val_loss: 1.2063\n",
      "Epoch 3/10\n",
      "20/20 - 45s - 2s/step - accuracy: 0.7047 - loss: 0.9117 - val_accuracy: 0.7063 - val_loss: 0.9157\n",
      "Epoch 4/10\n",
      "20/20 - 44s - 2s/step - accuracy: 0.7750 - loss: 0.7134 - val_accuracy: 0.7312 - val_loss: 0.8134\n",
      "Epoch 5/10\n",
      "20/20 - 44s - 2s/step - accuracy: 0.8406 - loss: 0.5348 - val_accuracy: 0.7625 - val_loss: 0.7187\n",
      "Epoch 6/10\n",
      "20/20 - 45s - 2s/step - accuracy: 0.8594 - loss: 0.4589 - val_accuracy: 0.7500 - val_loss: 0.7457\n",
      "Epoch 7/10\n",
      "20/20 - 45s - 2s/step - accuracy: 0.8906 - loss: 0.3802 - val_accuracy: 0.7875 - val_loss: 0.6368\n",
      "Epoch 8/10\n",
      "20/20 - 46s - 2s/step - accuracy: 0.9078 - loss: 0.3055 - val_accuracy: 0.8250 - val_loss: 0.6079\n",
      "Epoch 9/10\n",
      "20/20 - 46s - 2s/step - accuracy: 0.9047 - loss: 0.3032 - val_accuracy: 0.7625 - val_loss: 0.6331\n",
      "Epoch 10/10\n",
      "20/20 - 46s - 2s/step - accuracy: 0.9453 - loss: 0.2278 - val_accuracy: 0.7750 - val_loss: 0.6354\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x7d455c18d940>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_ds, epochs = 10, validation_data = val_ds, verbose = 2, batch_size = 20) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c03d1a87-c876-4c16-98cd-de10b34c6bd5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c164c3b6-1a99-45b8-8ff3-4e58386c511f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bb8c428a-b615-495c-b649-a06188a5b7e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 103ms/step\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "`decode_predictions` expects a batch of predictions (i.e. a 2D array of shape (samples, 1000)). Received array with shape: (1, 16)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# Predict\u001b[39;00m\n\u001b[32m      8\u001b[39m predictions = model.predict(img_array)\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m decoded_predictions = \u001b[43mdecode_predictions\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpredictions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m5\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[32m0\u001b[39m]\n\u001b[32m     11\u001b[39m \u001b[38;5;66;03m# Display results\u001b[39;00m\n\u001b[32m     12\u001b[39m plt.imshow(img)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/materias/ia/tp_decision_trees/ia_venv/lib/python3.12/site-packages/keras/src/applications/inception_v3.py:434\u001b[39m, in \u001b[36mdecode_predictions\u001b[39m\u001b[34m(preds, top)\u001b[39m\n\u001b[32m    432\u001b[39m \u001b[38;5;129m@keras_export\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mkeras.applications.inception_v3.decode_predictions\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    433\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecode_predictions\u001b[39m(preds, top=\u001b[32m5\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m434\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimagenet_utils\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdecode_predictions\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpreds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtop\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/materias/ia/tp_decision_trees/ia_venv/lib/python3.12/site-packages/keras/src/applications/imagenet_utils.py:136\u001b[39m, in \u001b[36mdecode_predictions\u001b[39m\u001b[34m(preds, top)\u001b[39m\n\u001b[32m    133\u001b[39m \u001b[38;5;28;01mglobal\u001b[39;00m CLASS_INDEX\n\u001b[32m    135\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(preds.shape) != \u001b[32m2\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m preds.shape[\u001b[32m1\u001b[39m] != \u001b[32m1000\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m136\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    137\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m`decode_predictions` expects \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    138\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33ma batch of predictions \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    139\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m(i.e. a 2D array of shape (samples, 1000)). \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    140\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mReceived array with shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpreds.shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    141\u001b[39m     )\n\u001b[32m    142\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m CLASS_INDEX \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    143\u001b[39m     fpath = file_utils.get_file(\n\u001b[32m    144\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mimagenet_class_index.json\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    145\u001b[39m         CLASS_INDEX_PATH,\n\u001b[32m    146\u001b[39m         cache_subdir=\u001b[33m\"\u001b[39m\u001b[33mmodels\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    147\u001b[39m         file_hash=\u001b[33m\"\u001b[39m\u001b[33mc2c37ea517e94d9795004a39431a14cb\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    148\u001b[39m     )\n",
      "\u001b[31mValueError\u001b[39m: `decode_predictions` expects a batch of predictions (i.e. a 2D array of shape (samples, 1000)). Received array with shape: (1, 16)"
     ]
    }
   ],
   "source": [
    "img_path = 'lsa16_segmented/1_1_1.png'  \n",
    "img = image.load_img(img_path, target_size=(299, 299)) # La carga en img y le hace resize a 299x299\n",
    "img_array = image.img_to_array(img)                    # La convierte a array de NumPy con dimensiones (299, 299, 3)\n",
    "img_array = np.expand_dims(img_array, axis=0)          # Agrega una dimension mas al array, haciendolo (1, 299, 299, 3) para batching\n",
    "img_array = preprocess_input(img_array)                # Matchea la representacion de la imagen a como la espera ImageNet (ej. mappea 0-255 a -1,1, cambia de RGB a BGR)\n",
    "\n",
    "# Predict\n",
    "predictions = model.predict(img_array)\n",
    "decoded_predictions = decode_predictions(predictions, top=5)[0]\n",
    "\n",
    "# Display results\n",
    "plt.imshow(img)\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "print(\"Top 5 Predictions:\")\n",
    "for i, (_, label, prob) in enumerate(decoded_predictions):\n",
    "    print(f\"{i + 1}: {label} ({prob * 100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6210e17d-f1dd-45bd-a9fb-79a79135dbf6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ia_venv)",
   "language": "python",
   "name": "ia_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
